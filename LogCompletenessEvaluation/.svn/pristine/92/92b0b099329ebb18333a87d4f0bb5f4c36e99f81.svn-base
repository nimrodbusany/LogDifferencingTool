Instructions:
=============
Import the LogCompletenessEvaluation project into Eclipse. 

Executing validation:
--------------------
MainValidation class contains the executable methods for running the validation process as it is described in section 6.2.3. 
validationForKTails(), validationForSynoptic() and validationForLSC() should be executed for k-tails, Synoptic and T/E respectively. 

The models used for validation can be managed through the ValidationModelsManager class. 
Each model should be specified along with its size and input trigger chart (in case t/e validation is going to be executed). 
By default, 14 synthetic models are used.

Executing evaluation:
--------------------
MainEvaluation class contains the executable methods for running the evaluation process as it is described in section 6.2.4. 
evaluationForKTails(), evaluationForSynoptic() and evaluationForLSC() should be executed for k-tails, Synoptic and T/E respectively. 

The models used for evaluation can be managed through the EvaluationModelsManager class. 
Each model should be specified along with its trace generation parameters (coverage type and number of visits) and input trigger chart (in case t/e evaluation is going to be executed). 
By default, 17 real life models are used as described in section 6.2.4.

* The modifiable evaluation/validation parameters and their default values are as follows:
number of trials (200), confidence threshold (0.95), initial number of traces (10), minimum length of each trace (0)
These parameters can be modified through the MainEvaluation class. 

* All generated logs will be placed under <project-directory>/resources/logs/{model-name}_generated_trace_{index}_by_{coverage}

* The validation/evaluation output, containing various measurements and statistics, will be printed to the console as well as exported to a CSV file under the folder: <project-directory>/resources/evaluation/[validation|evaluation]-{algorithm-name}.csv (e.g. validation-Synoptic.csv)
An additional CSV file containing the properties of the models (e.g. alphabet size and number of states in the model) will be created under the folder <project-directory>/resources/evaluation/[validation|evaluation]-{algorithm-name}-models.csv (e.g. validation-Synoptic-models.csv)


Executing example 1 (Shopping cart):
------------------------------------
DemoShoppingCartExample class contains a single executable method to run the shopping-cart example from section 2.1.
All three logs for this example (partial, full and redundant logs) are expected to be placed under the following directory: <project-directory>/resources/logs/shopping-cart


Executing example 2 (CrossFTP):
-------------------------------
DemoCrossFTPExample class contains a single executable method to run the CrossFTP Server example from section 2.2.
Both logs for this example (partial and full logs) are expected to be placed under the following directory: <project-directory>/resources/logs/crossftp


Executing example computation (open-read-close):
------------------------------------------------
DemoConfidenceComputation class contains a single executable method to run the confidence example computation from section 5.
The generated log can be found under: <project-directory>/resources/logs/ORC_generated_trace_0_by_states.txt
* Detailed output, including the full confidence breakdown for each trace, will be printed to the console as well as exported to the CSV file demo-confidence-breakdown.csv.
Two additional CSV files containing the properties of the model and other statistics will be created with the names demo-confidence-model.csv and demo-confidence.csv, respectively. 
Note that each execution of run() will result in a new randomly generated log from the open-real-close model, and therefore lead to different results and output. 

Executing LP validation:
------------------------
LPValidation class contains the executable methods for running the LP validation process as it is described in section 6.1.2. 
validationForKTails() and validationForSynoptic() should be executed for k-tails and Synoptic respectively. 
Note that the actual specification mining algorithm is being executed as part of the validation, and thus running time may be long. 
The output of the LP validation method is two models that were generated by the mining algorithm, exported to dot and PNG format. 
The generated models will be exported with the following name: {model-name}-[full|partial].[png|dot]
In order to create the PNG files, full path to dot.exe must be specified (edit the DOT_LOCATION constant). 
The full logs are expected to exist under <project-directory>/resources/logs/validation/{model-name}.txt prior to running LP validation. Minimal partial logs will be created from the full logs automatically. 
